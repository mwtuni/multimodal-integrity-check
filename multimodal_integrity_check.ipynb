{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Deployment: Wedding Day vs. GenAI\n",
    "\n",
    "### **The \"Wedding Day\": Traditional Computer Vision Models**\n",
    "- **Deployment is just the beginning.**\n",
    "  - Requires continuous updates, retraining, and maintenance.\n",
    "  - Long-term effort, much like sustaining a marriage.\n",
    "\n",
    "### **The \"Mistress\": Generative AI**\n",
    "- **Effortless but deceptive.**\n",
    "  - No continuous retraining required, appears easy and efficient.\n",
    "  - Pathological liar: hallucinates convincingly without a factual basis.\n",
    "\n",
    "### **The Solution: Multimodal Validation**\n",
    "- **Catch the lies before they spread.**\n",
    "  - Ensures all modalities (image, data, and prompt) are genuinely integrated.\n",
    "  - Detects hallucinations and enforces workflow trustworthiness.\n",
    "\n",
    "### **Takeaway:**\n",
    "\"Without validation, even the perfect wedding (or model) can fall apart.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Integrity Check for Generative AI\n",
    "This notebook demonstrates a common sense approach for multimodal integrity check using three input modalities:\n",
    "1. **Image (Modal 1):** `image.png`, representing the Jenkins character logo.\n",
    "2. **RAG Data (Modal 2):** Fictional facts about random characters (+Jenkins) provided in `data.txt`.\n",
    "3. **User Prompt (Modal 3):** A question asking about the character in the image and specific details like hair products.\n",
    "\n",
    "The goal is to verify that the generative AI pipeline integrates all three modalities: image, RAG data, and user prompt, to generate a coherent and enriched response. The test ensures the model does not compensate for a missing or failing modality by hallucinating its content. The successful integration of all three modalities must be evident in the generated response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Start HTTP Server for RAG Data\n",
    "The HTTP server serves the `data.txt` file for the RAG (Retrieval-Augmented Generation) pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP server started on port 8088. Access it at http://127.0.0.1:8088/\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Start a simple HTTP server to serve RAG data\n",
    "server = subprocess.Popen([\"python\", \"-m\", \"http.server\", \"8088\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"HTTP server started on port 8088. Access it at http://127.0.0.1:8088/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Multimodal Pipeline to be checked\n",
    "This cell implements the full multimodal pipeline using the Ollama framework. It integrates data from three modalities to produce a coherent and enriched response.\n",
    "\n",
    "### Workflow\n",
    "1. **RAG Data Processing:**\n",
    "   - Documents are retrieved from the HTTP server.\n",
    "   - Text is split into manageable chunks and indexed using a vector store.\n",
    "   - A retriever dynamically fetches relevant chunks based on the userâ€™s input.\n",
    "2. **Image Encoding:**\n",
    "   - The image is loaded and encoded in base64 format to be included in the prompt.\n",
    "3. **Integration:**\n",
    "   - The retrieved RAG data, user prompt, and encoded image are packaged into a structured input for the LLAVA visual-language model.\n",
    "4. **Output:**\n",
    "   - The LLAVA model processes the combined inputs and generates a detailed response.\n",
    "   - The response integrates visual, textual, and contextual information, demonstrating the pipeline's multimodal capability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\cfish\\AppData\\Local\\Temp\\ipykernel_19648\\4094518727.py:11: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  MODEL_VLM = ChatOllama(model=\"llava\")\n",
      "C:\\Users\\cfish\\AppData\\Local\\Temp\\ipykernel_19648\\4094518727.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  MODEL_EMBEDDING = embeddings.OllamaEmbeddings(model='nomic-embed-text')\n",
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The character in the image is Jenkins, an anthropomorphic character from the webcomic \"Jenkins.\" In the comic, Jenkins appears to be a sophisticated and suave man with a classic hairstyle. As for hair products, while I can't explicitly say what specific product Jenkins uses without more context or seeing the comic where this is mentioned, it's reasonable to assume that given his dapper appearance and the fact that he is drawn in a manner reminiscent of webcomics with attention to detail, Jenkins likely uses high-quality and possibly expensive hair products. This could include items such as pomade, hair gel, or styling mousse to maintain his slicked-back hairstyle. \n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Configuration variables\n",
    "MODEL_VLM = ChatOllama(model=\"llava\")\n",
    "MODEL_EMBEDDING = embeddings.OllamaEmbeddings(model='nomic-embed-text')\n",
    "PROMPT = \"\"\"\n",
    "Who is the character in the image, and what kind of hair products the character uses?\n",
    "\"\"\"\n",
    "\n",
    "def load_image(file_path):\n",
    "    \"\"\"Loads an image from the file path and encodes it in base64.\"\"\"\n",
    "    with open(file_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def load_and_split_documents(urls):\n",
    "    \"\"\"Loads documents from URLs and splits them into chunks.\"\"\"\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n",
    "    return text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Load RAG data from the HTTP server\n",
    "urls = [\"http://127.0.0.1:8088/data.txt\"]\n",
    "doc_splits = load_and_split_documents(urls)\n",
    "\n",
    "# Create a vector store\n",
    "vectorstore = Chroma.from_documents(documents=doc_splits, collection_name=\"rag-chroma\", embedding=MODEL_EMBEDDING)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Load the image\n",
    "image_b64 = load_image(r'image.png')\n",
    "\n",
    "# Function to integrate image, RAG data, and user prompt\n",
    "def prompt_func_rag(data):\n",
    "    query_results = retriever.invoke(data[\"text\"])\n",
    "    if query_results:\n",
    "        rag_content = \" \".join([doc.page_content for doc in query_results])\n",
    "    else:\n",
    "        rag_content = \"No relevant content found.\"\n",
    "\n",
    "    text = data[\"text\"]\n",
    "    image_part = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image_b64}\",\n",
    "    }\n",
    "    content_parts = [{\"type\": \"text\", \"text\": rag_content}, {\"type\": \"text\", \"text\": text}, image_part]\n",
    "\n",
    "    return [HumanMessage(content=content_parts)]\n",
    "\n",
    "# Run the pipeline\n",
    "chain = prompt_func_rag | MODEL_VLM | StrOutputParser()\n",
    "result = chain.invoke({\"rag\": retriever, \"text\": PROMPT, \"image\": image_b64})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Verifying the Generated Response\n",
    "This cell evaluates the output of the multimodal pipeline to ensure that it integrates information from all three input modalities and meets specific requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1) The character in the image is identified as Jenkins.\n",
      "(2) Yes, Jenkins uses coconut oil or blend as a hair product. The text specifically mentions that he uses this type of hair product and maintains his classic hairstyle.\n",
      "(3) The provided text coherently describes the character, Jenkins, his appearance, and his possible use of high-quality hair products such as coconut oil or blend. There are no missing facts or inconsistencies in the given response. \n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Create the messages list with hardcoded facts in the system prompt\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': (\n",
    "            \"You are a fact-checking AI assistant. Validate whether the given response: \"\n",
    "            \"(1) identifies the character as Jenkins, \"\n",
    "            \"(2) mentions that Jenkins uses coconut oil or blend as a hair product, \"\n",
    "            \"and (3) maintains coherence with the provided text. Highlight any missing facts or inconsistencies.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"This is the text to analyze:\\n\\n\" + result\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate a response\n",
    "response = ollama.chat(model='llava', messages=messages)\n",
    "\n",
    "# Print the response\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP server stopped.\n"
     ]
    }
   ],
   "source": [
    "# Shutdown HTTP server\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"HTTP server stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwtuni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
